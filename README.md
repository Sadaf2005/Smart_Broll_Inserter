Smart B-Roll Inserter for UGC Videos
ğŸ“Œ Overview

This project implements a Smart B-Roll Inserter system that automatically plans how B-roll clips should be inserted into an A-roll (talking-head / UGC) video.

Given:

One A-roll video (speaker talking to camera)

Multiple B-roll clips (product shots, lifestyle shots, etc.)

The system:

Understands what is being said and when in the A-roll

Understands what each B-roll clip represents

Uses semantic matching to decide:

where B-roll should be inserted

which B-roll fits best

Outputs a structured timeline plan in JSON format

Exposes the pipeline via FastAPI (Swagger UI) and a minimal React UI

The focus of this project is reasoning, system design, and correctness, not UI polish.

ğŸ§  System Architecture (High Level)
A-roll Video
   â†“
Audio Extraction (ffmpeg)
   â†“
Speech Transcription (Whisper)
   â†“
Transcript Segmentation (timestamps)

B-roll Metadata
   â†“
Semantic Descriptions

Transcript + B-roll Descriptions
   â†“
Embeddings (Sentence Transformers)
   â†“
Semantic Matching
   â†“
Timeline Planning
   â†“
JSON Timeline Plan

FastAPI (Swagger UI) + React UI

ğŸ› ï¸ Tech Stack & Justification
Backend
Technology	Why it is used
Python	Required by assignment, best ecosystem for ML, NLP, and video tooling
FastAPI	Lightweight, fast, auto-generated Swagger UI, ideal for ML pipelines
ffmpeg	Industry-standard tool for video/audio processing
Whisper (local)	Accurate timestamped transcription, avoids API quota limits
Sentence Transformers	Local semantic embeddings for matching speech to visuals
scikit-learn	Cosine similarity for semantic matching
Frontend
Technology	Why it is used
React	Required by assignment, simple UI for triggering pipeline and viewing output
Fetch API	Minimal dependency approach
UI Strategy

Swagger UI is intentionally used as a developer/editor interface to:

Trigger timeline generation

Inspect transcript, B-rolls, and final plan

React UI is kept minimal to demonstrate end-to-end integration

ğŸ“‚ Project Structure
smart-broll-inserter/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ api.py             # API routes
â”‚   â”‚   â”œâ”€â”€ schemas.py         # Data models
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ transcription.py
â”‚   â”‚   â”‚   â”œâ”€â”€ broll_analysis.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ matcher.py
â”‚   â”‚   â”‚   â””â”€â”€ planner.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ a_roll/
â”‚   â”‚   â”œâ”€â”€ b_roll/
â”‚   â”‚   â””â”€â”€ outputs/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ App.js
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md

ğŸ” Core Pipeline Explained
1ï¸âƒ£ A-Roll Understanding

Audio extracted from video using ffmpeg

Transcription generated with Whisper

Sentence-level timestamps retained for reasoning

2ï¸âƒ£ B-Roll Understanding

Provided metadata normalized into semantic text descriptions

Each B-roll represented as a textual concept

3ï¸âƒ£ Semantic Matching

Transcript segments and B-roll descriptions embedded

Cosine similarity used for semantic alignment

Best matching B-roll chosen per segment

Thresholds relaxed to handle Hinglish + abstract visuals

4ï¸âƒ£ Timeline Planning

Filters applied:

Minimum gap between insertions

Maximum number of B-rolls

Confidence threshold

Output includes:

Timestamp

Duration

B-roll ID

Confidence

Human-readable reason

ğŸ“¤ Output Format (Required)

Example output (timeline_plan.json):

{
  "total_insertions": 4,
  "insertions": [
    {
      "start_sec": 12.4,
      "duration_sec": 2.0,
      "broll_id": "broll_3",
      "confidence": 0.28,
      "reason": "B-roll selected because it visually reinforces the spoken content: 'Street food khate waqt hygiene...'"
    }
  ]
}

ğŸš€ How to Run the Project
ğŸ”§ Backend Setup
1ï¸âƒ£ Create & activate virtual environment
cd backend
python -m venv venv
venv\Scripts\activate   # Windows

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt
pip install uvicorn fastapi

3ï¸âƒ£ Make sure ffmpeg is installed
ffmpeg -version

â–¶ï¸ Start FastAPI Backend (Swagger UI)
cd backend
uvicorn app.main:app --reload


OR (fallback):

python -m uvicorn app.main:app --reload

Access:

Health check:
ğŸ‘‰ http://localhost:8000/health

Swagger UI:
ğŸ‘‰ http://localhost:8000/docs

From Swagger UI:

Use POST /generate-plan

Click Try it out â†’ Execute

View transcript, B-rolls, and timeline plan

ğŸŒ Frontend (React UI)
1ï¸âƒ£ Install dependencies
cd frontend
npm install

2ï¸âƒ£ Start React app
npm start

Access:

ğŸ‘‰ http://localhost:3000

Features:

Button to trigger timeline generation

Timeline JSON rendered in readable format

ğŸ¯ Design Trade-Offs & Decisions

Swagger UI used intentionally as a functional interface for inspecting system behavior

Local Whisper chosen to avoid API quota issues

Minimal React UI to demonstrate integration without overengineering

No video rendering UI (explicitly optional per assignment)

âœ… Assignment Requirements Coverage
Requirement	Status
Python backend	âœ…
ffmpeg usage	âœ…
Semantic matching	âœ…
Timeline JSON output	âœ…
React interface	âœ…
Swagger UI	âœ…
Video rendering	Optional (not implemented)
ğŸ“Œ Final Notes

This project focuses on:

Correctness

Explainability

System design clarity

Practical engineering trade-offs

It mirrors how such systems are evaluated internally before editor-facing tools are built.
